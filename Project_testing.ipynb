{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMMmGzjOsccYooBamFryueL",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/manojkumaarc/Capstone-Project-/blob/main/Project_testing.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# USER DATA COLLECTION"
      ],
      "metadata": {
        "id": "EF5XE7IkPSe5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pymongo"
      ],
      "metadata": {
        "id": "IbVLH4yOTyz8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install streamlit"
      ],
      "metadata": {
        "id": "xqUkP-WIUjOA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pyngrok"
      ],
      "metadata": {
        "id": "XLRTWnavUs1k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "GAAumCkoXXWm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile ytproject.py\n",
        "import streamlit as st\n",
        "st.write(\"YOUTUBE HARVESTING AND WAREHOUSING\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J85JvKarU0Oe",
        "outputId": "013ddaac-7cc7-473b-b8dc-ac0856767acf"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting ytproject.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!npm install localtunnel"
      ],
      "metadata": {
        "id": "Z_SRKc02VLkc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!streamlit run /content/app.py &>/content/logs.txt & npx localtunnel --port 8501 & curl ipv4.icanhazip.com"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Pt6Zg70pVRZ8",
        "outputId": "5247ee50-269e-4d0d-c1c7-dc7a8c7ddcf4"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "34.168.95.239\n",
            "\u001b[K\u001b[?25hnpx: installed 22 in 2.348s\n",
            "your url is: https://tasty-feet-melt.loca.lt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install streamlit_option_menu"
      ],
      "metadata": {
        "id": "leKcIqjfa81r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from googleapiclient.discovery import build\n",
        "import re\n",
        "import pymongo\n",
        "import streamlit as st\n",
        "from streamlit_option_menu import option_menu\n",
        "import pandas as pd\n",
        "import plotly.express as px"
      ],
      "metadata": {
        "id": "bEggrIGiUVz3"
      },
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# PAGE CONFIGURATION"
      ],
      "metadata": {
        "id": "JLx-5QjyYTxz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "st.set_page_config(page_title=\"YouTube Data Harvesting and Warehousing\",\n",
        "                   layout=\"wide\",\n",
        "                   initial_sidebar_state='auto',\n",
        "                   menu_items={'About':'Application developed by Manoj Kumaar',\n",
        "                   'About': \"This web application is used to analyze channel and video data from any youtube channel\"}\n",
        "                   )"
      ],
      "metadata": {
        "id": "jeExhD1aYQyG"
      },
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# NAVIGATION MENU"
      ],
      "metadata": {
        "id": "cUxZ1Q_mZbtv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "selected = option_menu(\n",
        "        menu_title = None,\n",
        "        options=[\"Home\",\"Data Processing\",\"Data Analysis\",\"About\"],\n",
        "        icons=[\"house-fill\",\"database-fill\",'graph-up-arrow',\"exclamation-lg\"],\n",
        "        default_index=0,\n",
        "        orientation=\"horizontal\",\n",
        "        styles={\n",
        "                \"container\": {\"background-color\": \"#000000\"},\n",
        "                \"icon\": {\"color\": \"white\", \"font-size\": \"25px\"},\n",
        "                \"nav-link\": {\"text-align\": \"centre\",\"--hover-color\": \"white\", \"color\" : \"white\"},\n",
        "                \"nav-link-selected\": {\"background-color\": \"red\"}\n",
        "               }\n",
        "       )\n",
        "\n",
        "st.header(\":red[YouTube Data Harvesting and Warehousing using SQLITE, MongoDB Atlas and Streamlit]\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2Gcb69btYQu8",
        "outputId": "e3206fea-8c26-46e7-97c4-d9d336a7a925"
      },
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2023-10-10 11:50:21.087 WARNING streamlit.runtime.state.session_state_proxy: Session state does not function when running a script without `streamlit run`\n",
            "2023-10-10 11:50:21.131 \n",
            "  \u001b[33m\u001b[1mWarning:\u001b[0m to view this Streamlit app on a browser, run it with the following\n",
            "  command:\n",
            "\n",
            "    streamlit run /usr/local/lib/python3.10/dist-packages/colab_kernel_launcher.py [ARGUMENTS]\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "DeltaGenerator()"
            ]
          },
          "metadata": {},
          "execution_count": 45
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# HOME PAGE"
      ],
      "metadata": {
        "id": "vqXvUGDWbiE6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "if selected == 'Home':\n",
        "    st.subheader('Home')\n",
        "\n",
        "    st.markdown(':red[**_YouTube Data Harvesting and Warehousing_**] is a Streamlit application that allows users to access and analyze data from multiple YouTube channels.')\n",
        "\n",
        "    st.write('''**It involves building a simple UI with Streamlit, retrieving data from the YouTube API, storing it in a MongoDB, migrating it to a SQLITE data warehouse, querying the data warehouse with SQL, and displaying the data in the Streamlit app.**''')\n",
        "\n",
        "    st.write(''':red[**_The application has the following features,_**]\\n\n",
        "    1. Based on user input for YouTube channel ID, all details related to all videos can be retrieved using Google API.\\n\n",
        "    2. Record the retreived data into MongoDB Atlas database.\\n\n",
        "    3. Option to select a channel name and migrate its data from MongoDB Atlas to a SQLite database as tables.\\n\n",
        "    4. Ability to search and retrieve data from the SQLite database using different search options to get channel details.''')\n",
        "\n",
        "    st.subheader('_Process_')\n",
        "\n",
        "    st.markdown('**:red[Step 1] :**  To collect the details of a youTube channel with the help of a Google API and channel ID.')\n",
        "\n",
        "    st.markdown('**:red[Step 2] :**  To process the data and record in MongoDB Atlas as JSON format (unstructured format).')\n",
        "\n",
        "    st.markdown('**:red[Step 3] :**  To fetch the data from MongoDB Atlas, and store it in a MySql database as tables.')\n",
        "\n",
        "    st.markdown('**:red[Step 4] :**  To develop a web application using Streamlit library of Python')\n",
        "\n",
        "    st.markdown('**:red[Step 5] :**  In the application we provide data visualization option to the user depending on the questions they choose')\n",
        "\n",
        "    st.markdown('**:red[Step 6] :**  Data Visualization is been done with the help of python libraries like plotly and pandas')\n",
        "\n",
        "    st.subheader('_Outcome_')\n",
        "\n",
        "    st.markdown('''Users can fetch the details of a YouTube channel by just entering the channel's ID and store it in both MongoDB Atlas and Sqlite. They can also analyse the data with 10 provided queries.''')"
      ],
      "metadata": {
        "id": "h3Uc3_43beeX"
      },
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# DATA PROCESSING AND COLLECTION"
      ],
      "metadata": {
        "id": "t8Qdydi-dYp3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "if selected == 'Data Processing':\n",
        "\n",
        "    #--------------------------------------------------Data Collection-----------------------------------------------\n",
        "    st.subheader('Data Collection')\n",
        "    channel_id = st.text_input(':red[**_Enter the channel_id :_**]')\n",
        "    Fetch_Data = st.button('**Fetch and Store**')\n",
        "\n",
        "    if Fetch_Data == True:\n",
        "        # Youtube API\n",
        "        api_service_name = 'youtube'\n",
        "        api_version = 'v3'\n",
        "        api_key = 'AIzaSyAKvPg0bnobb84jGgLoZkhOnei_csbbPAQ'\n",
        "        youtube = build(api_service_name,api_version,developerKey=api_key)\n",
        "\n",
        "        # Fetching channel data\n",
        "        def get_channel_data(youtube,channel_id):\n",
        "            channel_request = youtube.channels().list(\n",
        "                part = 'snippet,statistics,contentDetails',\n",
        "                id = channel_id)\n",
        "            channel_response = channel_request.execute()\n",
        "\n",
        "            if 'items' not in channel_response:\n",
        "                st.write(f\"Invalid channel id: {channel_id}\")\n",
        "                st.error(\"Enter the **channel_id**\")\n",
        "                return None\n",
        "\n",
        "            return channel_response\n",
        "\n",
        "        channel_data = get_channel_data(youtube,channel_id)\n",
        "\n",
        "        channel_name = channel_data['items'][0]['snippet']['title']\n",
        "        channel_video_count = channel_data['items'][0]['statistics']['videoCount']\n",
        "        channel_subscriber_count = channel_data['items'][0]['statistics']['subscriberCount']\n",
        "        channel_view_count = channel_data['items'][0]['statistics']['viewCount']\n",
        "        channel_playlist_id = channel_data['items'][0]['contentDetails']['relatedPlaylists']['uploads']\n",
        "\n",
        "        # Channel_data in dictionary\n",
        "\n",
        "        channel = {\n",
        "            \"Channel_Details\": {\n",
        "                \"Channel_Name\": channel_name,\n",
        "                \"Channel_Id\": channel_id,\n",
        "                \"Video_Count\": channel_video_count,\n",
        "                \"Subscriber_Count\": channel_subscriber_count,\n",
        "                \"Channel_Views\": channel_view_count,\n",
        "                \"Playlist_Id\": channel_playlist_id\n",
        "            }\n",
        "        }\n",
        "\n",
        "        # Fetching video IDs from channel playlist\n",
        "        def get_video_ids(youtube, channel_playlist_id):\n",
        "            video_id = []\n",
        "            next_page_token = None\n",
        "            while True:\n",
        "                # Get playlist items\n",
        "                request = youtube.playlistItems().list(\n",
        "                    part='contentDetails',\n",
        "                    playlistId=channel_playlist_id,\n",
        "                    maxResults=50,\n",
        "                    pageToken=next_page_token)\n",
        "                response = request.execute()\n",
        "\n",
        "                # Get video IDs\n",
        "                for item in response['items']:\n",
        "                    video_id.append(item['contentDetails']['videoId'])\n",
        "\n",
        "                # Check if there are more pages\n",
        "                next_page_token = response.get('nextPageToken')\n",
        "                if not next_page_token:\n",
        "                    break\n",
        "\n",
        "            return video_id\n",
        "\n",
        "        video_ids = get_video_ids(youtube, channel_playlist_id)\n",
        "\n",
        "        # Retrieving video data\n",
        "        def get_video_data(youtube, video_ids):\n",
        "            video_data = []\n",
        "            for video_id in video_ids:\n",
        "                try:\n",
        "                    request = youtube.videos().list(\n",
        "                    part='snippet, statistics, contentDetails',\n",
        "                    id=video_id)\n",
        "                    response = request.execute()\n",
        "\n",
        "                    video = response['items'][0]\n",
        "\n",
        "                    # Comments if available\n",
        "                    try:\n",
        "                     video['comment_threads'] = get_video_comments(youtube, video_id, max_comments=2)\n",
        "                    except:\n",
        "                        video['comment_threads'] = None\n",
        "\n",
        "                    # Duration format transformation (Duration format transformation function call)\n",
        "                    duration = video.get('contentDetails', {}).get('duration', 'Not Available')\n",
        "                    if duration != 'Not Available':\n",
        "                        duration = convert_duration(duration)\n",
        "                    video['contentDetails']['duration'] = duration\n",
        "\n",
        "                    video_data.append(video)\n",
        "\n",
        "                except:\n",
        "                    st.write('You have exceeded your YouTube API quota. Please try again tomorrow.')\n",
        "\n",
        "            return video_data\n",
        "\n",
        "        # Retrieving video comments\n",
        "        def get_video_comments(youtube, video_id, max_comments):\n",
        "            request = youtube.commentThreads().list(\n",
        "                part='snippet',\n",
        "                maxResults=max_comments,\n",
        "                textFormat=\"plainText\",\n",
        "                videoId=video_id)\n",
        "            response = request.execute()\n",
        "\n",
        "            return response\n",
        "\n",
        "        # Define a function to convert duration\n",
        "        def convert_duration(duration):\n",
        "            regex = r'PT(\\d+H)?(\\d+M)?(\\d+S)?'\n",
        "            match = re.match(regex, duration)\n",
        "            if not match:\n",
        "                return '00:00:00'\n",
        "            hours, minutes, seconds = match.groups()\n",
        "            hours = int(hours[:-1]) if hours else 0\n",
        "            minutes = int(minutes[:-1]) if minutes else 0\n",
        "            seconds = int(seconds[:-1]) if seconds else 0\n",
        "            total_seconds = hours * 3600 + minutes * 60 + seconds\n",
        "            return '{:02d}:{:02d}:{:02d}'.format(int(total_seconds / 3600), int((total_seconds % 3600) / 60), int(total_seconds % 60))\n",
        "\n",
        "        video_data = get_video_data(youtube, video_ids)\n",
        "\n",
        "\n",
        "        # Video details\n",
        "        videos = {}\n",
        "        for i, video in enumerate (video_data):\n",
        "            video_id = video['id']\n",
        "            video_name = video['snippet']['title']\n",
        "            published_at = video['snippet']['publishedAt']\n",
        "            view_count = video['statistics']['viewCount']\n",
        "            like_count = video['statistics'].get('likeCount', 0)\n",
        "            favorite_count = video['statistics'].get('favoriteCount', 0)\n",
        "            comment_count = video['statistics'].get('commentCount', 0)\n",
        "            duration = video.get('contentDetails', {}).get('duration', 'Not Available')\n",
        "            thumbnail = video['snippet']['thumbnails']['high']['url']\n",
        "            caption_status = video.get('contentDetails', {}).get('caption', 'Not Available')\n",
        "            comments = 'Unavailable'\n",
        "\n",
        "            # Handle case where comments are enabled\n",
        "            if video['comment_threads'] is not None:\n",
        "                comments = {}\n",
        "                for index, comment_thread in enumerate(video['comment_threads']['items']):\n",
        "                    comment = comment_thread['snippet']['topLevelComment']['snippet']\n",
        "                    comment_id = comment_thread['id']\n",
        "                    comment_text = comment['textDisplay']\n",
        "                    comment_author = comment['authorDisplayName']\n",
        "                    comment_published_at = comment['publishedAt']\n",
        "                    comments[f\"Comment_Id_{index + 1}\"] = {\n",
        "                        'Comment_Id': comment_id,\n",
        "                        'Comment_Text': comment_text,\n",
        "                        'Comment_Author': comment_author,\n",
        "                        'Comment_PublishedAt': comment_published_at\n",
        "                    }\n",
        "\n",
        "            # Video data into dictionary\n",
        "            videos[f\"Video_Id_{i + 1}\"] = {\n",
        "                'Video_Id': video_id,\n",
        "                'Video_Name': video_name,\n",
        "                'PublishedAt': published_at,\n",
        "                'View_Count': view_count,\n",
        "                'Like_Count': like_count,\n",
        "                'Favorite_Count': favorite_count,\n",
        "                'Comment_Count': comment_count,\n",
        "                'Duration': duration,\n",
        "                'Comments': comments\n",
        "            }\n",
        "\n",
        "        # Channel data and videos data to a dict\n",
        "        final_output = {**channel, **videos}\n"
      ],
      "metadata": {
        "id": "pscQZFFmdoip"
      },
      "execution_count": 48,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "MongoDB connection"
      ],
      "metadata": {
        "id": "fKY3UKVgf0Js"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "TtHZESK4PGny",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8a17e708-025c-4b77-ae57-d96aa460b6d5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "**Enter 11 digit channel_id**UClyHmPhz4kJw_zwCqYL5BLg\n"
          ]
        }
      ],
      "source": [
        "import googleapiclient.discovery\n",
        "import pandas as pd\n",
        "\n",
        "channel_id = input('**Enter 11 digit channel_id**')\n",
        "\n",
        "            # Access youtube API\n",
        "api_service_name = 'youtube'\n",
        "api_version = 'v3'\n",
        "api_key = 'AIzaSyAKvPg0bnobb84jGgLoZkhOnei_csbbPAQ'\n",
        "youtube = googleapiclient.discovery.build(api_service_name,api_version,developerKey =api_key)\n",
        "\n",
        "      # Define a function to retrieve channel data\n",
        "\n",
        "def get_channel_data(youtube,channel_id):\n",
        "      try:\n",
        "          try:\n",
        "              channel_request = youtube.channels().list(\n",
        "                  part = 'snippet,statistics,contentDetails',\n",
        "                  id = channel_id)\n",
        "              channel_response = channel_request.execute()\n",
        "\n",
        "              if 'items' not in channel_response:\n",
        "                  print(f\"Invalid channel id: {channel_id}\")\n",
        "                  print(\"Enter the correct 11-digit **channel_id**\")\n",
        "                  return None\n",
        "\n",
        "              return channel_response\n",
        "          except HttpError as e:\n",
        "                print('Server error (or) Check your internet connection (or) Please Try again after a few minutes', icon='ðŸš¨')\n",
        "                print('An error occurred: %s' % e)\n",
        "                return None\n",
        "      except:\n",
        "            print('You have exceeded your YouTube API quota. Please try again tomorrow.')\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "5ruuNtpCYEgD"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Function call to Get Channel data from a single channel ID\n",
        "channel_data = get_channel_data(youtube,channel_id)"
      ],
      "metadata": {
        "id": "d_2uV9HDRXkI"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Process channel data\n",
        "# Extract required information from the channel_data\n",
        "channel_name = channel_data['items'][0]['snippet']['title']\n",
        "channel_video_count = channel_data['items'][0]['statistics']['videoCount']\n",
        "channel_subscriber_count = channel_data['items'][0]['statistics']['subscriberCount']\n",
        "channel_view_count = channel_data['items'][0]['statistics']['viewCount']\n",
        "channel_playlist_id = channel_data['items'][0]['contentDetails']['relatedPlaylists']['uploads']"
      ],
      "metadata": {
        "id": "Tr3dLE0dTOd1"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "channel = {\n",
        "            \"Channel_Details\": {\n",
        "                \"Channel_Name\": channel_name,\n",
        "                \"Channel_Id\": channel_id,\n",
        "                \"Video_Count\": channel_video_count,\n",
        "                \"Subscriber_Count\": channel_subscriber_count,\n",
        "                \"Channel_Views\": channel_view_count,\n",
        "                 \"Playlist_Id\": channel_playlist_id\n",
        "            }\n",
        "        }"
      ],
      "metadata": {
        "id": "gC3nH86hTuXV"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define a function to retrieve video IDs from channel playlist\n",
        "def get_video_ids(youtube, channel_playlist_id):\n",
        "\n",
        "    video_id = []\n",
        "    next_page_token = None\n",
        "    while True:\n",
        "        # Get playlist items\n",
        "        request = youtube.playlistItems().list(\n",
        "            part='contentDetails',\n",
        "            playlistId=channel_playlist_id,\n",
        "            maxResults=50,\n",
        "            pageToken=next_page_token)\n",
        "        response = request.execute()\n",
        "\n",
        "        # Get video IDs\n",
        "        for item in response['items']:\n",
        "            video_id.append(item['contentDetails']['videoId'])\n",
        "\n",
        "        # Check if there are more pages\n",
        "        next_page_token = response.get('nextPageToken')\n",
        "        if not next_page_token:\n",
        "            break\n",
        "\n",
        "    return video_id\n",
        "\n",
        "# Function call to Get  video_ids using channel playlist Id\n",
        "video_ids = get_video_ids(youtube, channel_playlist_id)"
      ],
      "metadata": {
        "id": "qQ8rG-jXT4YQ"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        " # Define a function to retrieve video data\n",
        "def get_video_data(youtube, video_ids):\n",
        "\n",
        "    video_data = []\n",
        "    for video_id in video_ids:\n",
        "        try:\n",
        "            # Get video details\n",
        "            request = youtube.videos().list(\n",
        "                part='snippet, statistics, contentDetails',\n",
        "                id=video_id)\n",
        "            response = request.execute()\n",
        "\n",
        "            video = response['items'][0]\n",
        "\n",
        "            # Get comments if available (comment function call)\n",
        "            try:\n",
        "                video['comment_threads'] = get_video_comments(youtube, video_id, max_comments=2)\n",
        "            except:\n",
        "                video['comment_threads'] = None\n",
        "\n",
        "            # Duration format transformation (Duration format transformation function call)\n",
        "            duration = video.get('contentDetails', {}).get('duration', 'Not Available')\n",
        "            if duration != 'Not Available':\n",
        "                duration = convert_duration(duration)\n",
        "            video['contentDetails']['duration'] = duration\n",
        "\n",
        "            video_data.append(video)\n",
        "\n",
        "        except:\n",
        "            print('You have exceeded your YouTube API quota. Please try again tomorrow.')\n",
        "\n",
        "    return video_data"
      ],
      "metadata": {
        "id": "FrsC5jEeUSoT"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define a function to retrieve video comments\n",
        "def get_video_comments(youtube, video_id, max_comments):\n",
        "\n",
        "    request = youtube.commentThreads().list(\n",
        "        part='snippet',\n",
        "        maxResults=max_comments,\n",
        "        textFormat=\"plainText\",\n",
        "        videoId=video_id)\n",
        "    response = request.execute()\n",
        "\n",
        "    return response"
      ],
      "metadata": {
        "id": "4NfS4Ox3UyJJ"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        " # Define a function to convert duration\n",
        "def convert_duration(duration):\n",
        "  regex = r'PT(\\d+H)?(\\d+M)?(\\d+S)?'\n",
        "  match = re.match(regex, duration)\n",
        "  if not match:\n",
        "      return '00:00:00'\n",
        "  hours, minutes, seconds = match.groups()\n",
        "  hours = int(hours[:-1]) if hours else 0\n",
        "  minutes = int(minutes[:-1]) if minutes else 0\n",
        "  seconds = int(seconds[:-1]) if seconds else 0\n",
        "  total_seconds = hours * 3600 + minutes * 60 + seconds\n",
        "  return '{:02d}:{:02d}:{:02d}'.format(int(total_seconds / 3600), int((total_seconds % 3600) / 60), int(total_seconds % 60))"
      ],
      "metadata": {
        "id": "b0t6wHWeU5tB"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Function call to Get Videos data and comment data from video ids\n",
        "video_data = get_video_data(youtube, video_ids)"
      ],
      "metadata": {
        "id": "xUomfNkrVHz7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bbdde4e6-c0c6-4ab5-bf77-b683ea5c1b74"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "You have exceeded your YouTube API quota. Please try again tomorrow.\n",
            "You have exceeded your YouTube API quota. Please try again tomorrow.\n",
            "You have exceeded your YouTube API quota. Please try again tomorrow.\n",
            "You have exceeded your YouTube API quota. Please try again tomorrow.\n",
            "You have exceeded your YouTube API quota. Please try again tomorrow.\n",
            "You have exceeded your YouTube API quota. Please try again tomorrow.\n",
            "You have exceeded your YouTube API quota. Please try again tomorrow.\n",
            "You have exceeded your YouTube API quota. Please try again tomorrow.\n",
            "You have exceeded your YouTube API quota. Please try again tomorrow.\n",
            "You have exceeded your YouTube API quota. Please try again tomorrow.\n",
            "You have exceeded your YouTube API quota. Please try again tomorrow.\n",
            "You have exceeded your YouTube API quota. Please try again tomorrow.\n",
            "You have exceeded your YouTube API quota. Please try again tomorrow.\n",
            "You have exceeded your YouTube API quota. Please try again tomorrow.\n",
            "You have exceeded your YouTube API quota. Please try again tomorrow.\n",
            "You have exceeded your YouTube API quota. Please try again tomorrow.\n",
            "You have exceeded your YouTube API quota. Please try again tomorrow.\n",
            "You have exceeded your YouTube API quota. Please try again tomorrow.\n",
            "You have exceeded your YouTube API quota. Please try again tomorrow.\n",
            "You have exceeded your YouTube API quota. Please try again tomorrow.\n",
            "You have exceeded your YouTube API quota. Please try again tomorrow.\n",
            "You have exceeded your YouTube API quota. Please try again tomorrow.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##OCTOBER 10"
      ],
      "metadata": {
        "id": "Ljkf201RVSsq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# video details processing\n",
        "videos = {}\n",
        "for i, video in enumerate (video_data):\n",
        "    video_id = video['id']\n",
        "    video_name = video['snippet']['title']\n",
        "    published_at = video['snippet']['publishedAt']\n",
        "    view_count = video['statistics']['viewCount']\n",
        "    like_count = video['statistics'].get('likeCount', 0)\n",
        "    favorite_count = video['statistics'].get('favoriteCount', 0)\n",
        "    comment_count = video['statistics'].get('commentCount', 0)\n",
        "    duration = video.get('contentDetails', {}).get('duration', 'Not Available')\n",
        "    comments = 'Unavailable'\n"
      ],
      "metadata": {
        "id": "VKct_PSqVXvO"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Handle case where comments are enabled\n",
        "# if video['comment_threads'] is not None:\n",
        "#     comments = {}\n",
        "#     for index, comment_thread in enumerate(video['comment_threads']['items']):\n",
        "#         comment = comment_thread['snippet']['topLevelComment']['snippet']\n",
        "#         comment_id = comment_thread['id']\n",
        "#         comment_text = comment['textDisplay']\n",
        "#         comment_author = comment['authorDisplayName']\n",
        "#         comment_published_at = comment['publishedAt']\n",
        "#         comments[f\"Comment_Id_{index + 1}\"] = {\n",
        "#             'Comment_Id': comment_id,\n",
        "#             'Comment_Text': comment_text,\n",
        "#             'Comment_Author': comment_author,\n",
        "#             'Comment_PublishedAt': comment_published_at\n",
        "#         }"
      ],
      "metadata": {
        "id": "lBgQ57vjVs36"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Format processed video data into dictionary\n",
        "videos[f\"Video_Id_{i + 1}\"] = {     'Video_Id': video_id,\n",
        "                                    'Video_Name': video_name,\n",
        "                                    'PublishedAt': published_at,\n",
        "                                    'View_Count': view_count,\n",
        "                                    'Like_Count': like_count,\n",
        "                                    'Favorite_Count': favorite_count,\n",
        "                                    'Comment_Count': comment_count,\n",
        "                                    'Duration': duration,\n",
        "                                    'Comments': comments\n",
        "                                }"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 245
        },
        "id": "eQWL_F9uWDy9",
        "outputId": "2dac0743-ad72-4027-a407-48f46b6ccec2"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-20-8a61349e0492>\u001b[0m in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Format processed video data into dictionary\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m videos[f\"Video_Id_{i + 1}\"] = {\n\u001b[0;32m----> 3\u001b[0;31m                                     \u001b[0;34m'Video_Id'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mvideo_id\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m                                     \u001b[0;34m'Video_Name'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mvideo_name\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m                                     \u001b[0;34m'PublishedAt'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mpublished_at\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'video_id' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        " # Channel data and videos data to a dict\n",
        "        final_output = {**channel, **videos}"
      ],
      "metadata": {
        "id": "7mNlYJX3SmLU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "MongoDB connection and store the collected data"
      ],
      "metadata": {
        "id": "cQtNCTf4Sqz9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        " client = pymongo.MongoClient('mongodb+srv://cmanoj1:12345@cluster0.zbhnebl.mongodb.net/')\n",
        " mydb = client['ytcapstone']\n",
        " collection = mydb['channel']\n",
        " final_output_data = {\n",
        "            # 'Channel_Name': channel_name,\n",
        "            # \"Channel_data\":final_output\n",
        "            \"a\":\"1\"\n",
        "            }\n",
        "upload = collection.replace_one({'_id': 1234}, final_output_data, upsert=True)\n",
        "#st.write(f\"Updated document id: {upload.upserted_id if upload.upserted_id else upload.modified_count}\")\n",
        "client.close()"
      ],
      "metadata": {
        "id": "ywFLr5RTSm8m"
      },
      "execution_count": 75,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "YM3qbQ2wpOxk"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "mSj4T7PapOQt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Import module\n",
        "import sqlite3\n",
        "\n",
        "# Connecting to sqlite\n",
        "conn = sqlite3.connect('yt.db')\n",
        "\n",
        "# Creating a cursor object using the\n",
        "# cursor() method\n",
        "cursor = conn.cursor()\n",
        "\n",
        "# # Creating table\n",
        "# table =\"\"\"CREATE TABLE STUDENT(NAME VARCHAR(255), CLASS VARCHAR(255),\n",
        "# SECTION VARCHAR(255));\"\"\"\n",
        "# cursor.execute(table)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "axfffd9-ivMY",
        "outputId": "7f4f9964-715b-467b-fb5a-81781b439364"
      },
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<sqlite3.Cursor at 0x7e254a6c91c0>"
            ]
          },
          "metadata": {},
          "execution_count": 56
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "a= cursor.execute(\"PRAGMA table_info('STUDENT')\")\n",
        "\n",
        "for i in a:\n",
        "\n",
        "     print(i)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IWr1LJpsj_XB",
        "outputId": "d07c2a6d-d165-4719-f2f9-8a3fc925cc96"
      },
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(0, 'NAME', 'VARCHAR(255)', 0, None, 0)\n",
            "(1, 'CLASS', 'VARCHAR(255)', 0, None, 0)\n",
            "(2, 'SECTION', 'VARCHAR(255)', 0, None, 0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = pd.DataFrame({'Names':['Abhinav','Aryan',\n",
        "                                 'Manthan'],\n",
        "                        'DOB' : ['10/01/2009','24/03/2009',\n",
        "                                '28/02/2009']})"
      ],
      "metadata": {
        "id": "tdztUB9ukuV5"
      },
      "execution_count": 59,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#importing sql library\n",
        "from sqlalchemy import create_engine\n",
        "\n",
        "# create a reference\n",
        "# for sql library\n",
        "engine = create_engine('sqlite:////content/yt.db',\n",
        "                       echo = False)\n",
        "conn = engine.connect()\n",
        "# attach the data frame to the sql\n",
        "# with a name of the table\n",
        "# as \"Employee_Data\"\n",
        "# dataset.to_sql('Employee_Data',\n",
        "              #  con = engine)\n",
        "\n",
        "# show the complete data\n",
        "# from Employee_Data table\n",
        "result = cursor.execute('''SELECT * FROM Employee_Data''')\n",
        "for i in result:\n",
        "  print(i)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bcDFASRJk0y_",
        "outputId": "c415cbd9-f1c4-4235-a10a-63ddddb46940"
      },
      "execution_count": 73,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(0, 'Abhinav', '10/01/2009')\n",
            "(1, 'Aryan', '24/03/2009')\n",
            "(2, 'Manthan', '28/02/2009')\n"
          ]
        }
      ]
    }
  ]
}